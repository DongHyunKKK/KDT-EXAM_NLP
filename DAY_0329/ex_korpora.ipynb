{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자연어 데이터셋 전처리 <hr>\n",
    "- Korpora 패키지 활용\n",
    "- 데이터셋 : 한국어 혐오 데이터셋 korean_hate_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting Korpora\n",
      "  Downloading Korpora-0.2.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting dataclasses>=0.6 (from Korpora)\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from Korpora) (1.24.3)\n",
      "Requirement already satisfied: tqdm>=4.46.0 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from Korpora) (4.65.0)\n",
      "Requirement already satisfied: requests>=2.20.0 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from Korpora) (2.31.0)\n",
      "Collecting xlrd>=1.2.0 (from Korpora)\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from requests>=2.20.0->Korpora) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from requests>=2.20.0->Korpora) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from requests>=2.20.0->Korpora) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from requests>=2.20.0->Korpora) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from tqdm>=4.46.0->Korpora) (0.4.6)\n",
      "Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.8 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 30.7/57.8 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 57.8/57.8 kB 609.5 kB/s eta 0:00:00\n",
      "Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "   ---------------------------------------- 0.0/96.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 96.5/96.5 kB 1.8 MB/s eta 0:00:00\n",
      "Installing collected packages: dataclasses, xlrd, Korpora\n",
      "Successfully installed Korpora-0.2.0 dataclasses-0.6 xlrd-2.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script korpora.exe is installed in 'C:\\Users\\user\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "# 패키지 설치\n",
    "# !pip install Korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 패키지 설치 확인\n",
    "import Korpora\n",
    "\n",
    "Korpora.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[korean hate speech] download unlabeled_comments_1.txt: 57.0MB [00:11, 4.93MB/s]                              \n",
      "[korean hate speech] download unlabeled_comments_2.txt: 59.3MB [00:11, 5.03MB/s]                            \n",
      "[korean hate speech] download unlabeled_comments_3.txt: 58.9MB [00:11, 4.93MB/s]                              \n",
      "[korean hate speech] download unlabeled_comments_4.txt: 58.2MB [00:12, 4.77MB/s]                              \n",
      "[korean hate speech] download unlabeled_comments_5.txt: 3.57MB [00:00, 3.61MB/s]                            \n",
      "[korean hate speech] download unlabeled_comments.news_title_1.txt: 44.7MB [00:08, 5.03MB/s]                            \n",
      "[korean hate speech] download unlabeled_comments.news_title_2.txt: 47.4MB [00:09, 4.91MB/s]                            \n",
      "[korean hate speech] download unlabeled_comments.news_title_3.txt: 48.5MB [00:09, 4.99MB/s]                            \n",
      "[korean hate speech] download unlabeled_comments.news_title_4.txt: 47.9MB [00:09, 5.04MB/s]                            \n",
      "[korean hate speech] download unlabeled_comments.news_title_5.txt: 3.30MB [00:00, 3.55MB/s]                            \n",
      "[korean hate speech] download dev.news_title.txt: 49.2kB [00:00, 101kB/s]                             \n",
      "[korean hate speech] download test.news_title.txt: 98.3kB [00:00, 175kB/s]                             \n",
      "[korean hate speech] download train.news_title.txt: 745kB [00:00, 1.27MB/s]                            \n",
      "[korean hate speech] download dev.tsv: 57.3kB [00:00, 115kB/s]                             \n",
      "[korean hate speech] download train.tsv: 918kB [00:00, 1.05MB/s]                            \n",
      "[korean hate speech] download test.no_label.tsv: 98.3kB [00:00, 175kB/s]                             \n"
     ]
    }
   ],
   "source": [
    "# 데이터 다운로드\n",
    "from Korpora import Korpora\n",
    "\n",
    "# 데이터셋 파일명\n",
    "data_file = 'korean_hate_speech'\n",
    "\n",
    "# 다운로드 fetch\n",
    "corpus = Korpora.fetch(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Authors :\n",
      "        - Jihyung Moon* (inmoonlight@github)\n",
      "        - Won Ik Cho* (warnikchow@github)\n",
      "        - Junbum Lee (beomi@github)\n",
      "        * equal contribution\n",
      "    Repository : https://github.com/kocohub/korean-hate-speech\n",
      "    References :\n",
      "        - Moon, J., Cho, W. I., & Lee, J. (2020). BEEP! Korean Corpus of Online News\n",
      "          Comments for Toxic Speech Detection. arXiv preprint arXiv:2005.12503.\n",
      "\n",
      "    We provide the first human-annotated Korean corpus for toxic speech detection and the large unlabeled corpus.\n",
      "    The data is comments from the Korean entertainment news aggregation platform.\n",
      "\n",
      "    # License\n",
      "    Creative Commons Attribution-ShareAlike 4.0 International License.\n",
      "    Visit here for detail : https://creativecommons.org/licenses/by-sa/4.0/\n",
      "\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\user\\Korpora\\korean_hate_speech\\unlabeled\\unlabeled_comments_1.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\user\\Korpora\\korean_hate_speech\\unlabeled\\unlabeled_comments_2.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\user\\Korpora\\korean_hate_speech\\unlabeled\\unlabeled_comments_3.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\user\\Korpora\\korean_hate_speech\\unlabeled\\unlabeled_comments_4.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\user\\Korpora\\korean_hate_speech\\unlabeled\\unlabeled_comments_5.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\user\\Korpora\\korean_hate_speech\\news_title\\unlabeled_comments.news_title_1.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\user\\Korpora\\korean_hate_speech\\news_title\\unlabeled_comments.news_title_2.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\user\\Korpora\\korean_hate_speech\\news_title\\unlabeled_comments.news_title_3.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\user\\Korpora\\korean_hate_speech\\news_title\\unlabeled_comments.news_title_4.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\user\\Korpora\\korean_hate_speech\\news_title\\unlabeled_comments.news_title_5.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\user\\Korpora\\korean_hate_speech\\news_title\\dev.news_title.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\user\\Korpora\\korean_hate_speech\\news_title\\test.news_title.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\user\\Korpora\\korean_hate_speech\\news_title\\train.news_title.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\user\\Korpora\\korean_hate_speech\\labeled\\dev.tsv\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\user\\Korpora\\korean_hate_speech\\labeled\\train.tsv\n",
      "[Korpora] Corpus `korean hate speech` is already installed at C:\\Users\\user\\Korpora\\korean_hate_speech\\test.no_label.tsv\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로딩\n",
    "corpus = Korpora.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(현재 호텔주인 심정) 아18 난 마른하늘에 날벼락맞고 호텔망하게생겼는데 누군 계속 추모받네....\n",
      "\"밤새 조문 행렬…故 전미선, 동료들이 그리워하는 따뜻한 배우 [종합]\"\n",
      "False\n",
      "others\n",
      "SentencePair(text='\"[단독] 지드래곤♥이주연, 제주도 데이트…2018년 1호 커플 탄생\"', pair='\"[단독] 지드래곤♥이주연, 제주도 데이트…2018년 1호 커플 탄생\"')\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "hateDS = corpus.train\n",
    "hate_valDS = corpus.dev\n",
    "hate_testDS = corpus.test\n",
    "unlabelDS = corpus.unlabeled\n",
    "\n",
    "# 1개 데이터 정보 추출\n",
    "print(f'{hateDS[0].text}')\n",
    "print(f'{hateDS[0].title}')\n",
    "print(f'{hateDS[0].gender_bias}')\n",
    "print(f'{hateDS[0].bias}')\n",
    "\n",
    "print(unlabelDS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 데이터 전처리 <hr>\n",
    "- 주제에 따른 데이터 피처 & 라벨 선택 ===> 뉴스댓글 + 차별종류\n",
    "- 형태소 분석기 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피처 데이터 추출\n",
    "textData = corpus.get_all_texts()\n",
    "\n",
    "with open ('../DATA/corpus.txt', 'w', encoding = 'utf-8') as f:\n",
    "    for text in textData:\n",
    "        f.write(text + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp38-cp38-win_amd64.whl.metadata (8.3 kB)\n",
      "Downloading sentencepiece-0.2.0-cp38-cp38-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 10.2/991.7 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 92.2/991.7 kB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 573.4/991.7 kB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  983.0/991.7 kB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 991.7/991.7 kB 5.2 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceTrainer\n",
    "\n",
    "SentencePieceTrainer.Train(input = '../DATA/corpus.txt',\n",
    "                           model_prefix = 'text_bpe',\n",
    "                           vocab_size = 8000,\n",
    "                           model_type = 'bpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "tokenizer = SentencePieceProcessor()\n",
    "tokenizer.load('text_bpe.model')\n",
    "\n",
    "sentence = '안녕하세요, 토크나이저가 잘 학습되었군요!'\n",
    "sentences = ['이렇게 입력값 리스트로 받아서', '쉽게 토크나이저를 사용할 수 있답니다.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단일 문장 토큰화 : ['▁안녕', '하세요', ',', '▁토크', '나이', '저', '가', '▁잘', '▁학', '습', '되', '었', '군', '요', '!']\n",
      "단일 문장 토큰화 : [['▁이렇게', '▁입', '력', '값', '▁리', '스트', '로', '▁받아', '서'], ['▁', '쉽', '게', '▁토크', '나이', '저', '를', '▁사용', '할', '▁수', '▁있', '답', '니다', '.']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence = tokenizer.encode_as_pieces(sentence)\n",
    "tokenized_sentences = tokenizer.encode_as_pieces(sentences)\n",
    "\n",
    "print('단일 문장 토큰화 :', tokenized_sentence)\n",
    "print('여러 문장 토큰화 :', tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단일 문장 정수 인코딩 : [4598, 1073, 6681, 3614, 2850, 6901, 6711, 290, 816, 7105, 7102, 6929, 7149, 6812, 6799]\n",
      "여러 문장 정수 인코딩 : [[1646, 116, 7000, 7541, 533, 314, 6709, 1561, 6726], [6677, 7525, 6806, 3614, 2850, 6901, 7124, 3689, 6886, 95, 146, 7195, 264, 6679]]\n"
     ]
    }
   ],
   "source": [
    "encoded_sentence = tokenizer.encode_as_ids(sentence)\n",
    "encoded_sentences = tokenizer.encode_as_ids(sentences)\n",
    "\n",
    "print('단일 문장 정수 인코딩 :', encoded_sentence)\n",
    "print('여러 문장 정수 인코딩 :', encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩에서 문장 변환: ['이렇게 입력값 리스트로 받아서', '쉽게 토크나이저를 사용할 수 있답니다.']\n",
      "하위 단어 토큰에서 문장 변환: ['이렇게 입력값 리스트로 받아서', '쉽게 토크나이저를 사용할 수 있답니다.']\n"
     ]
    }
   ],
   "source": [
    "decode_ids = tokenizer.decode_ids(encoded_sentences)\n",
    "decode_pieces = tokenizer.decode_pieces(encoded_sentences)\n",
    "\n",
    "print('정수 인코딩에서 문장 변환:', decode_ids)\n",
    "print('하위 단어 토큰에서 문장 변환:', decode_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, '\"\"'), (4, '..')]\n",
      "vocab size : 8000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SentencePieceProcessor()\n",
    "tokenizer.load('text_bpe.model')\n",
    "\n",
    "vocab = {idx : tokenizer.id_to_piece(idx) for idx in range(tokenizer.get_piece_size())}\n",
    "print(list(vocab.items())[:5])\n",
    "print('vocab size :', len(vocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
