{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataPipe 타입 >>> iterator 타입 형변환\n",
    "train_iter = iter(AG_NEWS(split = 'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 확인 => (label, text), label : 1 ~ 4\n",
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토커나이즈 생성\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# 뉴스 학습 데이터 추출\n",
    "train_iter = AG_NEWS(split = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words_list = stopwords.words('english')\n",
    "print('불용어 개수 :', len(stop_words_list))\n",
    "print('불용어 10개 출력 :',stop_words_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 제너레이터 함수 : 데이터 추출하여 토큰화\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        # 라벨, 텍스트 => 텍스트 토큰화\n",
    "        text_split = [word for word in text.split() if word not in stop_words_list + list(string.punctuation) + ['$']]\n",
    "        text = ' '.join(text_split)\n",
    "        text = re.sub('[^A-Za-z0-9가-힣]', ' ', text)  # 한글, 영어, 숫자만 남기고 특수문자 제거\n",
    "        float_pattern = r\"\\b\\d+\\.\\d+\\b\"  # 실수 제거\n",
    "        text = re.sub(float_pattern, \"\", text)\n",
    "        text = re.sub(r'\\d', '', text)  # 정수 제거\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "for _, text in train_iter:\n",
    "    print(_, text)\n",
    "    print(type(text))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ['wall', 'st', 'bears', 'claw', 'back', 'into', 'black', 'reuters', 'reuters', 'short', 'sellers', 'wall', 'street', 's', 'dwindling', 'band', 'ultra', 'cynics', 'seeing', 'green', 'again']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for text in yield_tokens(train_iter):\n",
    "    print(_, text)\n",
    "    print(type(text))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어사전 생성\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials = ['<pad>', '<unk>'])\n",
    "\n",
    "# <UNK> 인덱스 0으로 설정\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "# <PAD> 인덱스 0으로 설정\n",
    "vocab.set_default_index(vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61872"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 => 정수 인코딩\n",
    "text_pipeline = lambda x : vocab(tokenizer(x))\n",
    "\n",
    "# 레이블 => 정수 인코딩\n",
    "label_pipeline = lambda x : int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 배치 크기만큼 데이터셋 반환 함수\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "\n",
    "    # 1개씩 뉴스기사, 라벨 추출해서 저장\n",
    "    for (_label, _text) in batch:\n",
    "        # 라벨 인코딩 후 저장\n",
    "        label_list.append(label_pipeline(_label))\n",
    "\n",
    "        # 텍스트 인코딩 후 저장\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype = torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "\n",
    "        # 텍스트 offset 즉, 텍스트 크기/길이 저장\n",
    "        offsets.append(processed_text.size(0))\n",
    "    \n",
    "    label_list = torch.tensor(label_list, dtype = torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim = 0)\n",
    "    text_list = torch.cat(text_list)\n",
    "\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split = 'train')\n",
    "dataloader = DataLoader(train_iter,\n",
    "                        batch_size = 8,\n",
    "                        shuffle = True,\n",
    "                        collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num class : 4   vocab size : 61872\n"
     ]
    }
   ],
   "source": [
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f'num class : {num_class}   vocab size : {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 2, 3, 0, 3, 2]) tensor([ 8308,   217,   487, 23015,     0,     9,   666,  5164,   104,  3455,\n",
      "          104,     3,  1256,   440,     3,    16,     0,   578,  5091,     3,\n",
      "           15,    63,  2057,  4626,    15,    65,     0,   314,   188,  2190,\n",
      "          173,     0,  1057, 23015,     0,    55,  2270,  1164,   152,  2097,\n",
      "            0,  1738,     0,   232,  1136,     0,   314,  1726,  5494,   123,\n",
      "          737,   440,   119,    23,  1375,  1738,   315,   232,    86,   979,\n",
      "          104,  5455,   809, 20367,   483,   821,   115,   104,     8,  2717,\n",
      "          612,     3,  6097,  6087,   429,  1289,   701,     0,     0,     0,\n",
      "         4322, 29454,   358,   452,     0,     2,     0,  5396,   306,     0,\n",
      "            6,     0,     0,   137,     0,     2, 12144, 29454, 14735,    63,\n",
      "            8,   999,  1567,   104,    68,     3,   452,     0,     2,     0,\n",
      "         2934,  5396,   217,   626,   245,     3,   306,   169,    25,     0,\n",
      "         7831,    97,  1038,     0,   132,   210, 18543,   159, 23526,  7831,\n",
      "          125,     0,     0, 17609,     0,    50,   435,     0,   443,     0,\n",
      "          153,     0,   155,    23,     5,   509,    97,   272,     0,     0,\n",
      "            0,    59,     0,  9901,     8,     0,   503,  2594,     0,   283,\n",
      "          392,   188,     4,   441,   819,   486,     4,  1999,     0,  3428,\n",
      "            0,   706,    63,     3,  1510,   819,  9147,     8,  1982,   183,\n",
      "         5182, 61241,     3,   426,    19,   821,    78,   819,     0,     2,\n",
      "            0,  1510,  8809,   104,  3305,   315,   828,  3718,    63,   137,\n",
      "            0,  1712,  7884, 10137,  1098,     0,     2,   416,  1712,  7884,\n",
      "         3302,   469,  1098,   155,    23,   183, 16218,  2648,   315, 11508,\n",
      "          440,  1380,   483,  9153,  1609,     0,  1958,   662,   356,   673,\n",
      "          315,  4431, 11448,   155,     3, 16100,  9976,   947,     0,   144,\n",
      "         8154,   955,   104,  2140,   440,   564,  5454,    39,     4,   153,\n",
      "          834,   483,    55,  1135,     3,    76,   564,   117,  2410,     0,\n",
      "          314,   978, 41611,  2221,    63,    75,   193, 13478,     0,   909,\n",
      "           39,    55,   821,   211,   310,   280,   869,     0,  1477,   909,\n",
      "          247,   995,    26,     0,   106,  2724,   404,   499,   283,   300,\n",
      "          440,   826,   791,   104,  1203,   618,     0,   112,   269,  9754,\n",
      "        13692,     5,    31,    63,     8,  6715,   104,   967,     0]) tensor([  0,  41,  80, 120, 164, 201, 239, 269])\n"
     ]
    }
   ],
   "source": [
    "for labels, texts, offsets in dataloader:\n",
    "    print(labels, texts, offsets)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__getitem__(): incompatible function arguments. The following argument types are supported:\n    1. (self: torchtext._torchtext.Vocab, arg0: str) -> int\n\nInvoked with: <torchtext._torchtext.Vocab object at 0x0000019978E6B870>, 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## 인코딩 : 문자 >>> 숫자로 변환\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m token_to_id \u001b[38;5;241m=\u001b[39m {voca : \u001b[38;5;28mid\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, voca \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(vocab)}\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m## 디코딩 : 숫자 >>> 문자로 변환\u001b[39;00m\n\u001b[0;32m      5\u001b[0m id_to_token \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mid\u001b[39m : voca \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, voca \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(vocab)}\n",
      "Cell \u001b[1;32mIn[82], line 2\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## 인코딩 : 문자 >>> 숫자로 변환\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m token_to_id \u001b[38;5;241m=\u001b[39m {voca : \u001b[38;5;28mid\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, voca \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(vocab)}\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m## 디코딩 : 숫자 >>> 문자로 변환\u001b[39;00m\n\u001b[0;32m      5\u001b[0m id_to_token \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mid\u001b[39m : voca \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, voca \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(vocab)}\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\Torch_NLP38\\lib\\site-packages\\torchtext\\vocab\\vocab.py:65\u001b[0m, in \u001b[0;36mVocab.__getitem__\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mexport\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, token: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m        token: The token used to lookup the corresponding index.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m        The index corresponding to the associated token.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: __getitem__(): incompatible function arguments. The following argument types are supported:\n    1. (self: torchtext._torchtext.Vocab, arg0: str) -> int\n\nInvoked with: <torchtext._torchtext.Vocab object at 0x0000019978E6B870>, 0"
     ]
    }
   ],
   "source": [
    "## 인코딩 : 문자 >>> 숫자로 변환\n",
    "token_to_id = {voca : id for id, voca in enumerate(vocab)}\n",
    "\n",
    "## 디코딩 : 숫자 >>> 문자로 변환\n",
    "id_to_token = {id : voca for id, voca in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 인코딩 : 문자 >>> 숫자로 변환\n",
    "token_to_id = {voca : id for id, voca in enumerate(vocab)}\n",
    "\n",
    "## 디코딩 : 숫자 >>> 문자로 변환\n",
    "id_to_token = {id : voca for id, voca in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰에 문자를 정수로 변환 및 단어/어휘 사전에 없는 문자도 처리\n",
    "UNK_ID = token_to_id.get('<UNK>')\n",
    "\n",
    "train_ids = [[token_to_id.get(token, UNK_ID) for token in text] for text in train_tokens]\n",
    "test_ids = [[token_to_id.get(token, UNK_ID) for token in text] for text in test_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 처리 함수\n",
    "\n",
    "# - sentences : 토큰화된 문장 데이터\n",
    "# - max_length : 최대 문장길이 즉, 1개 문장 구성 단어수\n",
    "# - pad : 패딩 처리 시 추가될 문자 값\n",
    "# - start : 패딩 시 처리 방향 [기:R 오른쪽 즉, 뒷부분 자르기/추가하기]\n",
    "def pad_sequence(sentences, max_length, pad, start = 'R'):\n",
    "    result = []\n",
    "    for sen in sentences:\n",
    "        sen = sen[:max_length] if start == 'R' else sen[:-1*max_length]\n",
    "        padd_sen = sen + [pad]*(max_length-len(sen)) if start == 'R' else [pad]*(max_length-len(sen)) + sen\n",
    "        result.append(padd_sen)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용, 테스트용 데이터 패딩 처리\n",
    "PAD_ID = token_to_id.get('<PAD>')\n",
    "MAX_LENGTH = 32\n",
    "\n",
    "train_ids = pad_sequence(train_ids, MAX_LENGTH, PAD_ID)\n",
    "test_ids = pad_sequence(test_ids, MAX_LENGTH, PAD_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성 : List를 Tensor로 변환\n",
    "\n",
    "# 학습용 데이터셋\n",
    "dataTS = torch.LongTensor(train_ids)\n",
    "labelTS = torch.FloatTensor(trainDF.label.values)\n",
    "\n",
    "print(f'dataTS => {dataTS.shape}, labelTS => {labelTS.shape}')\n",
    "\n",
    "trainDS = TensorDataset(dataTS, labelTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트용 데이터셋\n",
    "dataTS = torch.LongTensor(test_ids)\n",
    "labelTS = torch.FloatTensor(testDF.label.values)\n",
    "\n",
    "print(f'dataTS => {dataTS.shape}, labelTS => {labelTS.shape}')\n",
    "\n",
    "testDS = TensorDataset(dataTS, labelTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로더 생성\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "trainDL = DataLoader(trainDS, BATCH_SIZE, shuffle = True)\n",
    "testDL = DataLoader(testDS, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
