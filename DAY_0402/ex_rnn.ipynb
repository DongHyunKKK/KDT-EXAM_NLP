{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [순환신경망 RNN] <hr> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 1\n",
    "NUM_LAYERS = 1\n",
    "SEQ_LENGTH = 3\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# 데이터 및 초기 hidden state\n",
    "input = torch.randn(1, 3, 10)  # 입력데이터(배치크기, 시퀀스길이, 피처길이)\n",
    "\n",
    "# 첫번째 hidden state 초기값\n",
    "h0 = torch.randn(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE)  # 히든 초기값(양방향*층수, 배치크기, 히든개수)\n",
    "\n",
    "# RNN 인스턴스 생성\n",
    "rnn = nn.RNN(10, HIDDEN_SIZE, NUM_LAYERS, batch_first = True)\n",
    "\n",
    "# RNN 출력\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN 출력 - input\n",
      "-SHAPE ; torch.Size([1, 3, 10])  DIM : 3D\n",
      "tensor([[[-0.6457],\n",
      "         [-0.9697],\n",
      "         [-0.8609]]], grad_fn=<TransposeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(f'RNN 출력 - input\\n-SHAPE ; {input.shape}  DIM : {input.ndim}D')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN 출력 - output\n",
      "- SHAPE ; torch.Size([1, 3, 1])  DIM : 3D\n"
     ]
    }
   ],
   "source": [
    "print(f'RNN 출력 - output\\n- SHAPE ; {output.shape}  DIM : {output.ndim}D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN 출력 - hidden state\n",
      "- SHAPE : torch.Size([1, 1, 1])  DIM : 3D\n"
     ]
    }
   ],
   "source": [
    "print(f'RNN 출력 - hidden state\\n- SHAPE : {hn.shape}  DIM : {hn.ndim}D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RNN PARAMETERS]\n",
      "---------[weight_ih_l0] \n",
      "Parameter containing:\n",
      "tensor([[-0.0231,  0.4460,  0.5643,  0.7584, -0.9757,  0.5525, -0.7677,  0.2187,\n",
      "         -0.0482,  0.7619]], requires_grad=True)]\n",
      "---------[weight_hh_l0] \n",
      "Parameter containing:\n",
      "tensor([[0.7301]], requires_grad=True)]\n",
      "---------[bias_ih_l0] \n",
      "Parameter containing:\n",
      "tensor([0.2808], requires_grad=True)]\n",
      "---------[bias_hh_l0] \n",
      "Parameter containing:\n",
      "tensor([0.8415], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print('[RNN parameters]')\n",
    "for name, param in rnn.named_parameters():\n",
    "    print(f'---------[{name}] \\n{param}]') \n",
    "# 같은 층에 있는 HS끼리 상호작용해서 절편 계산 ex) HIDDEN_SIZE = 3 : bias 수 = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[all_weights] : 1\n",
      "[[Parameter containing:\n",
      "tensor([[-0.0231,  0.4460,  0.5643,  0.7584, -0.9757,  0.5525, -0.7677,  0.2187,\n",
      "         -0.0482,  0.7619]], requires_grad=True), Parameter containing:\n",
      "tensor([[0.7301]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2808], requires_grad=True), Parameter containing:\n",
      "tensor([0.8415], requires_grad=True)]]\n"
     ]
    }
   ],
   "source": [
    "# rnn 모델의 속성 출력\n",
    "print(f'[all_weights] : {len(rnn.all_weights)}')\n",
    "print(rnn.all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설계 : 다층 RNN, 층 2개\n",
    "\n",
    "# 입력 초기 텐서 2개\n",
    "input = torch.randn(1, 4, 10)  # (배치크기, 시퀀스(문장의 단어 수), 피처수(단어 표현 벡터 길이))\n",
    "h0 = torch.randn(1, 1, 5)      # (양방향*층수, 배치크기, 은닉상태 사이즈) => 은닉상태 초기화\n",
    "    \n",
    "# RNN 인스턴스\n",
    "rnn = nn.RNN(10, 5, 1, batch_first = True)\n",
    "\n",
    "# 출력 텐서 2개\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "RNN                                      85\n",
       "=================================================================\n",
       "Total params: 85\n",
       "Trainable params: 85\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[all_weight] - 1개\n",
      "[[Parameter containing:\n",
      "tensor([[-0.1778, -0.4063,  0.0246,  0.1611, -0.1057,  0.2479,  0.2652, -0.0958,\n",
      "         -0.2414,  0.3913],\n",
      "        [-0.3686,  0.2867,  0.3046,  0.3439, -0.3139, -0.2496,  0.1323, -0.3502,\n",
      "         -0.1681, -0.1763],\n",
      "        [ 0.3077, -0.1015, -0.2791, -0.2766, -0.0420,  0.1717, -0.2844, -0.2312,\n",
      "          0.0167,  0.1308],\n",
      "        [ 0.0647,  0.2333,  0.1081,  0.1551, -0.4444,  0.2606, -0.0060,  0.0054,\n",
      "         -0.1526, -0.1833],\n",
      "        [ 0.0625,  0.1250, -0.3363, -0.0012, -0.2375,  0.2495, -0.2147, -0.2569,\n",
      "         -0.0400, -0.3665]], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.1546, -0.0384,  0.2214,  0.0385, -0.1643],\n",
      "        [ 0.2052,  0.0226, -0.4339, -0.3078,  0.2671],\n",
      "        [-0.4418, -0.4228, -0.1525, -0.0277, -0.2026],\n",
      "        [-0.1767,  0.0503, -0.2043, -0.3920,  0.2399],\n",
      "        [-0.1399, -0.1305,  0.2617, -0.3790,  0.2600]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.4354,  0.3090, -0.0830, -0.1409, -0.4388], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4009,  0.1888, -0.1849, -0.2868, -0.2276], requires_grad=True)]]\n"
     ]
    }
   ],
   "source": [
    "# RNN 모델의 속성 출력\n",
    "print(f'[all_weight] - {len(rnn.all_weights)}개')\n",
    "print(rnn.all_weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 5]),\n",
       " 3,\n",
       " tensor([[[-0.4777, -0.2731,  0.4484, -0.8532, -0.9261],\n",
       "          [-0.0364, -0.7533,  0.6997, -0.8998, -0.9047],\n",
       "          [-0.4178,  0.5859,  0.0097,  0.1019,  0.1667],\n",
       "          [ 0.5796, -0.8032,  0.5620, -0.9517, -0.8086]]],\n",
       "        grad_fn=<TransposeBackward1>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RNN 출력 텐서 output\n",
    "output.shape, output.ndim, output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
