{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [순환신경망 RNN] <hr> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\Torch_NLP38\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\ProgramData\\anaconda3\\envs\\Torch_NLP38\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\ProgramData\\anaconda3\\envs\\Torch_NLP38\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 1\n",
    "NUM_LAYERS = 1\n",
    "SEQ_LENGTH = 3\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# 데이터 및 초기 hidden state\n",
    "input = torch.randn(1, 3, 10)  # 입력데이터(배치크기, 시퀀스길이, 피처길이)\n",
    "\n",
    "# 첫번째 hidden state 초기값\n",
    "h0 = torch.randn(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE)  # 히든 초기값(양방향*층수, 배치크기, 히든개수)\n",
    "\n",
    "# RNN 인스턴스 생성\n",
    "rnn = nn.RNN(10, HIDDEN_SIZE, NUM_LAYERS, batch_first = True)\n",
    "\n",
    "# RNN 출력\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN 출력 - input\n",
      "-SHAPE ; torch.Size([1, 3, 10])  DIM : 3D\n",
      "tensor([[[-0.8937],\n",
      "         [ 1.0000],\n",
      "         [ 0.9706]]], grad_fn=<TransposeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(f'RNN 출력 - input\\n-SHAPE ; {input.shape}  DIM : {input.ndim}D')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN 출력 - output\n",
      "- SHAPE ; torch.Size([1, 3, 1])  DIM : 3D\n"
     ]
    }
   ],
   "source": [
    "print(f'RNN 출력 - output\\n- SHAPE ; {output.shape}  DIM : {output.ndim}D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN 출력 - hidden state\n",
      "- SHAPE : torch.Size([1, 1, 1])  DIM : 3D\n"
     ]
    }
   ],
   "source": [
    "print(f'RNN 출력 - hidden state\\n- SHAPE : {hn.shape}  DIM : {hn.ndim}D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RNN parameters]\n",
      "---------[weight_ih_l0] \n",
      "Parameter containing:\n",
      "tensor([[-0.2801,  0.5060,  0.3781,  0.8186, -0.0749,  0.7480, -0.8409,  0.9773,\n",
      "          0.9864, -0.7522]], requires_grad=True)]\n",
      "---------[weight_hh_l0] \n",
      "Parameter containing:\n",
      "tensor([[-0.3206]], requires_grad=True)]\n",
      "---------[bias_ih_l0] \n",
      "Parameter containing:\n",
      "tensor([0.0843], requires_grad=True)]\n",
      "---------[bias_hh_l0] \n",
      "Parameter containing:\n",
      "tensor([0.4089], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print('[RNN parameters]')\n",
    "for name, param in rnn.named_parameters():\n",
    "    print(f'---------[{name}] \\n{param}]') \n",
    "# 같은 층에 있는 HS끼리 상호작용해서 절편 계산 ex) HIDDEN_SIZE = 3 : bias 수 = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[all_weights] : 1\n",
      "[[Parameter containing:\n",
      "tensor([[-0.2801,  0.5060,  0.3781,  0.8186, -0.0749,  0.7480, -0.8409,  0.9773,\n",
      "          0.9864, -0.7522]], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.3206]], requires_grad=True), Parameter containing:\n",
      "tensor([0.0843], requires_grad=True), Parameter containing:\n",
      "tensor([0.4089], requires_grad=True)]]\n"
     ]
    }
   ],
   "source": [
    "# rnn 모델의 속성 출력\n",
    "print(f'[all_weights] : {len(rnn.all_weights)}')\n",
    "print(rnn.all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설계 : 다층 RNN, 층 2개\n",
    "\n",
    "# 입력 초기 텐서 2개\n",
    "input = torch.randn(1, 4, 10)  # (배치크기, 시퀀스(문장의 단어 수), 피처수(단어 표현 벡터 길이))\n",
    "h0 = torch.randn(1, 1, 5)      # (양방향*층수, 배치크기, 은닉상태 사이즈) => 은닉상태 초기화\n",
    "    \n",
    "# RNN 인스턴스\n",
    "rnn = nn.RNN(10, 5, 1, batch_first = True)\n",
    "\n",
    "# 출력 텐서 2개\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "RNN                                      85\n",
       "=================================================================\n",
       "Total params: 85\n",
       "Trainable params: 85\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[all_weight] - 1개\n",
      "[[Parameter containing:\n",
      "tensor([[ 0.1958,  0.0342,  0.1034,  0.2363,  0.0756, -0.0588, -0.3562,  0.1562,\n",
      "         -0.2359,  0.2448],\n",
      "        [-0.1322,  0.1767, -0.1304,  0.3581,  0.0546,  0.2111, -0.2511,  0.3301,\n",
      "         -0.3411, -0.1392],\n",
      "        [ 0.2734,  0.0196, -0.0172, -0.3432, -0.4229, -0.1259,  0.3792,  0.2397,\n",
      "         -0.3848, -0.3218],\n",
      "        [-0.0103,  0.3612,  0.2424, -0.1678,  0.3566, -0.3632, -0.0066,  0.3210,\n",
      "         -0.3941,  0.2751],\n",
      "        [ 0.1973,  0.0683,  0.4365, -0.1351, -0.3765, -0.0821,  0.1483, -0.1286,\n",
      "          0.3877, -0.3159]], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.1741,  0.0997, -0.1331,  0.2875, -0.4213],\n",
      "        [ 0.2911,  0.3310, -0.1712,  0.4128, -0.3041],\n",
      "        [-0.1420,  0.4036, -0.3939,  0.2749,  0.3197],\n",
      "        [-0.1662, -0.1371, -0.2393, -0.2761, -0.3180],\n",
      "        [ 0.0038,  0.1257, -0.0192,  0.3208,  0.0854]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4394,  0.2575, -0.1999,  0.1439, -0.3287], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0402,  0.1101, -0.4047, -0.3458, -0.2550], requires_grad=True)]]\n"
     ]
    }
   ],
   "source": [
    "# RNN 모델의 속성 출력\n",
    "print(f'[all_weight] - {len(rnn.all_weights)}개')\n",
    "print(rnn.all_weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 5]),\n",
       " 3,\n",
       " tensor([[[-0.6005, -0.0812, -0.3265, -0.8692, -0.3442],\n",
       "          [-0.0205,  0.4382, -0.9329,  0.6194, -0.9430],\n",
       "          [ 0.4551,  0.7747, -0.7934,  0.1805, -0.7322],\n",
       "          [ 0.0805,  0.7653, -0.9527, -0.3127, -0.6572]]],\n",
       "        grad_fn=<TransposeBackward1>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RNN 출력 텐서 output\n",
    "output.shape, output.ndim, output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
