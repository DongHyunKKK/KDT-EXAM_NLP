{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [한글 데이터셋 RNN] <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로딩\n",
    "import pandas as pd\n",
    "from Korpora import Korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\user\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\user\\Korpora\\nsmc\\ratings_test.txt\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    50000 non-null  object\n",
      " 1   label   50000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기 및 저장\n",
    "corpus = Korpora.load('nsmc')\n",
    "nscmDF = pd.DataFrame(corpus.test)\n",
    "nscmDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = nscmDF.sample(frac = 0.9, random_state = 42)\n",
    "testDF = nscmDF.drop(trainDF.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 45000 entries, 33553 to 6838\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    45000 non-null  object\n",
      " 1   label   45000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.0+ MB\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5000 entries, 9 to 49997\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    5000 non-null   object\n",
      " 1   label   5000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 117.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# 학습용, 테스트용 데이터 분리\n",
    "trainDF = nscmDF.sample(frac=0.9, random_state = 42)\n",
    "trainDF.info()\n",
    "print()\n",
    "testDF = nscmDF.drop(trainDF.index)\n",
    "testDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       | text                                                                                     |   label |\n",
      "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
      "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
      "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
      "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
      "| 12447 | 잔잔 격동                                                                                |       1 |\n",
      "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
      "Training Data Size : 45000\n",
      "Testing Data Size : 5000\n"
     ]
    }
   ],
   "source": [
    "print(trainDF.head(5).to_markdown())\n",
    "print('Training Data Size :', len(trainDF))\n",
    "print('Testing Data Size :', len(testDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 인스턴스 생성.\n",
    "tokenizer = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 단어로 분리\n",
    "# for text in trainDF.text :\n",
    "#     print(tokenizer.morphs(text, stem=True))\n",
    "#     break\n",
    "\n",
    "train_tokens = [tokenizer.morphs(text, stem = True) for text in trainDF.text] #stem = True 하면 사전형으로 어휘사전 vocab에 등록\n",
    "test_tokens = [tokenizer.morphs(text, stem = True) for text in testDF.text] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train_token] 45000개\n",
      "[test_tokens] : 5000개\n",
      "\n",
      "[train_token[0]] 19개\n",
      "[test_tokens[0]] : 18개\n",
      "\n",
      "[train_token[1]] 14개\n",
      "[test_tokens[1]] : 6개\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'[train_token] {len(train_tokens)}개\\n[test_tokens] : {len(test_tokens)}개\\n')\n",
    "print(f'[train_token[0]] {len(train_tokens[0])}개\\n[test_tokens[0]] : {len(test_tokens[0])}개\\n')\n",
    "print(f'[train_token[1]] {len(train_tokens[1])}개\\n[test_tokens[1]] : {len(test_tokens[1])}개\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어사전 생성함수\n",
    "def build_vocab(corpus, vocab_size, special_tokens):\n",
    "    counter = Counter()\n",
    "\n",
    "    # 단어/토큰에 대한 빈도수 계산\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # 단어/어휘 사전 생성\n",
    "    vocab = special_tokens\n",
    "\n",
    "    # 단어/어휘 사전에 빈도수가 높은 단어 추가\n",
    "    for token, count in counter.most_common(vocab_size):\n",
    "        vocab.append(token)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 데이터셋 단어/어휘 사전 생성\n",
    "VOCAB_SIZE = 5000\n",
    "VOCAB = build_vocab(corpus = train_tokens, vocab_size = VOCAB_SIZE, special_tokens = [\"<PAD>\", \"<UNK>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VOCAB] ===> 5002개\n",
      "['<PAD>', '<UNK>', '.', '이', '영화', '보다', '하다', '의', '..', '에', '가', '...', '을', '도', '들', ',', '는', '를', '은', '없다', '이다', '있다', '좋다', '?', '너무', '다', '정말', '한', '되다', '재밌다']\n"
     ]
    }
   ],
   "source": [
    "print(f'[VOCAB] ===> {len(VOCAB)}개\\n{VOCAB[:30]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 인코딩 : 문자 >>> 숫자로 변환\n",
    "token_to_id = {voca : id for id, voca in enumerate(VOCAB)}\n",
    "\n",
    "## 디코딩 : 숫자 >>> 문자로 변환\n",
    "id_to_token = {id : voca for id, voca in enumerate(VOCAB)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['모', '든'],\n",
       " ['편', '견'],\n",
       " ['을'],\n",
       " ['날', '다'],\n",
       " ['버', '리', '다'],\n",
       " ['가', '슴'],\n",
       " ['따', '뜻', '하', '다'],\n",
       " ['영', '화'],\n",
       " ['.'],\n",
       " ['로', '버', '트'],\n",
       " ['드'],\n",
       " ['니'],\n",
       " ['로'],\n",
       " [','],\n",
       " ['필', '립'],\n",
       " ['세', '이', '모', '어'],\n",
       " ['호', '프', '만'],\n",
       " ['영', '원', '하', '다'],\n",
       " ['.']]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[token for token in text] for text in train_tokens[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3-1] 토큰 정수화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰에 문자를 정수로 변환 및 단어/어휘 사전에 없는 문자도 처리\n",
    "UNK_ID = token_to_id.get('<UNK>')\n",
    "\n",
    "train_ids = [[token_to_id.get(token, UNK_ID) for token in text] for text in train_tokens]\n",
    "test_ids = [[token_to_id.get(token, UNK_ID) for token in text] for text in test_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3-2] 데이터 구성 단어 수 맞추기 즉, 패딩(padding)\n",
    "- 단어수 선정 필요\n",
    "- 선정된 단어 수에 맞게 데이터 조절 => 길면 잘라내기, 짧으면 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 처리 함수\n",
    "\n",
    "# - sentences : 토큰화된 문장 데이터\n",
    "# - max_length : 최대 문장길이 즉, 1개 문장 구성 단어수\n",
    "# - pad : 패딩 처리 시 추가될 문자 값\n",
    "# - start : 패딩 시 처리 방향 [기:R 오른쪽 즉, 뒷부분 자르기/추가하기]\n",
    "def pad_sequence(sentences, max_length, pad, start = 'R'):\n",
    "    result = []\n",
    "    for sen in sentences:\n",
    "        sen = sen[:max_length] if start == 'R' else sen[:-1*max_length]\n",
    "        padd_sen = sen + [pad]*(max_length-len(sen)) if start == 'R' else [pad]*(max_length-len(sen)) + sen\n",
    "        result.append(padd_sen)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용, 테스트용 데이터 패딩 처리\n",
    "PAD_ID = token_to_id.get('<PAD>')\n",
    "MAX_LENGTH = 32\n",
    "\n",
    "train_ids = pad_sequence(train_ids, MAX_LENGTH, PAD_ID)\n",
    "test_ids = pad_sequence(test_ids, MAX_LENGTH, PAD_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train ids] ===> 32개\n",
      "[test ids] ===> 32개\n"
     ]
    }
   ],
   "source": [
    "print(f'[train ids] ===> {len(train_ids[0])}개')\n",
    "print(f'[test ids] ===> {len(test_ids[0])}개')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] 학습 준비 <hr>\n",
    "- 데이터로더 준비\n",
    "- 학습용/테스트용 함수\n",
    "- 모델 클래스\n",
    "- 학습 관련 변수 => DEVICE, OPTIMIZER, MODEL 인스턴스, EPOCHS, BATCH SIZE, LOSS_FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4-1] 데이터 로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 32])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelTS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataTS => torch.Size([45000, 32]), labelTS => torch.Size([45000])\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 생성 : List를 Tensor로 변환\n",
    "\n",
    "# 학습용 데이터셋\n",
    "dataTS = torch.LongTensor(train_ids)\n",
    "labelTS = torch.FloatTensor(trainDF.label.values)\n",
    "\n",
    "print(f'dataTS => {dataTS.shape}, labelTS => {labelTS.shape}')\n",
    "\n",
    "trainDS = TensorDataset(dataTS, labelTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataTS => torch.Size([5000, 32]), labelTS => torch.Size([5000])\n"
     ]
    }
   ],
   "source": [
    "# 테스트용 데이터셋\n",
    "dataTS = torch.LongTensor(test_ids)\n",
    "labelTS = torch.FloatTensor(testDF.label.values)\n",
    "\n",
    "print(f'dataTS => {dataTS.shape}, labelTS => {labelTS.shape}')\n",
    "\n",
    "testDS = TensorDataset(dataTS, labelTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로더 생성\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "trainDL = DataLoader(trainDS, BATCH_SIZE, shuffle = True)\n",
    "testDL = DataLoader(testDS, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4-2] 모델 클래스\n",
    "- 입력층 : Embedding Layer\n",
    "- 은닉층 : RNN/LSTM Layer, Dropout Layer\n",
    "- 출력층 : Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_vocab,\n",
    "            hidden_dim,\n",
    "            embedding_dim,\n",
    "            n_layers,\n",
    "            dropout = 0.5,\n",
    "            bidirectional = True,\n",
    "            model_type = 'lstm'\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings = n_vocab,\n",
    "            embedding_dim = embedding_dim,\n",
    "            padding_idx = 0\n",
    "        )\n",
    "\n",
    "        if model_type == 'rnn':\n",
    "            self.model = nn.RNN(\n",
    "                input_size = embedding_dim,\n",
    "                hidden_size = hidden_dim,\n",
    "                num_layers = n_layers,\n",
    "                bidirectional = bidirectional,\n",
    "                dropout = dropout,\n",
    "                batch_first = True,\n",
    "            )     \n",
    "        elif model_type == 'lstm':\n",
    "            self.model = nn.LSTM(\n",
    "                input_size = embedding_dim,\n",
    "                hidden_size = hidden_dim,\n",
    "                num_layers = n_layers,\n",
    "                bidirectional = bidirectional,\n",
    "                dropout = dropout,\n",
    "                batch_first = True,\n",
    "            )\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "n_vocab = len(token_to_id)\n",
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "classifier = SentenceClassifier(n_vocab = n_vocab, hidden_dim = hidden_dim, embedding_dim = embedding_dim,\n",
    "                                n_layers = n_layers).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train(model, datasets, criterion, optimizer, device, interval):\n",
    "    model.train()\n",
    "    losses = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval == 0:\n",
    "            print(f'Train Loss {step} : {np.mean(losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, datasets, criterion, device):\n",
    "    model.eval()\n",
    "    losses = list()\n",
    "    corrects = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat = torch.sigmoid(logits) >.5\n",
    "        corrects.extend(torch.eq(yhat, labels).cpu().tolist())\n",
    "    \n",
    "    print(f'Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 0.6988293528556824\n",
      "Train Loss 500 : 0.6672000739269865\n",
      "Train Loss 1000 : 0.5919460575956922\n",
      "Val Loss : 0.46528638586117205, Val Accuracy : 0.7926\n",
      "Train Loss 0 : 0.4705561399459839\n",
      "Train Loss 500 : 0.4181335525895783\n",
      "Train Loss 1000 : 0.40979841387236154\n",
      "Val Loss : 0.40226447449368274, Val Accuracy : 0.8178\n",
      "Train Loss 0 : 0.4062395691871643\n",
      "Train Loss 500 : 0.34451214987003875\n",
      "Train Loss 1000 : 0.3469868473090849\n",
      "Val Loss : 0.38881009817123413, Val Accuracy : 0.8168\n",
      "Train Loss 0 : 0.3715892732143402\n",
      "Train Loss 500 : 0.2999616081188777\n",
      "Train Loss 1000 : 0.30430211150622394\n",
      "Val Loss : 0.4108718299562005, Val Accuracy : 0.827\n",
      "Train Loss 0 : 0.27594271302223206\n",
      "Train Loss 500 : 0.2651703634602343\n",
      "Train Loss 1000 : 0.26806588622775823\n",
      "Val Loss : 0.3878645946265786, Val Accuracy : 0.829\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "interval = 500\n",
    "for epoch in range(epochs):\n",
    "    train(classifier, trainDL, criterion, optimizer, device, interval)\n",
    "    test(classifier, testDL, criterion, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
