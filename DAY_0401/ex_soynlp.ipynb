{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Soynlp] 학습 기반 토크나이저 <hr>\n",
    "- 품사 태깅, 단어 토큰화 등을 지원하는 단어 토크나이저\n",
    "- 비지도 학습으로 단어 토큰화 ==> 데이터에 자주 등장하는 단어들을 단어로 분석\n",
    "- 내부적으로 단어 점수 표로 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install soynlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기존 형태소 분석기는 신조어나 형태소 분석기에 등록되지 않은 단어는 제대로 구분하지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['에이', '비식스', '이대', '휘', '1월', '최애', '돌', '기부', '요정', '입니다']\n",
      "['에이', '비식스', '이대', '휘', '1', '우러', '최애', '돌', '기부', '요정', '이다']\n",
      "['에이', '비식스', '이대', '휘', '1', '우러', '최애', '돌', '기부', '요정', '입니다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "tokenizer = Okt()\n",
    "\n",
    "print(tokenizer.morphs('에이비식스 이대휘 1월 최애돌 기부 요정 입니다'))\n",
    "\n",
    "# 형태소 분석시 매개변수 stem = True 설정\n",
    "print(tokenizer.morphs('에이비식스 이대휘 1우러 최애돌 기부 요정 입니다', stem = True))\n",
    "\n",
    "print(tokenizer.morphs('에이비식스 이대휘 1우러 최애돌 기부 요정 입니다', norm = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Sonlpy] 사용 => 말뭉치 데이터셋으로 학습 진행 후 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../DATA/text_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 처리\n",
    "from soynlp import DoublespaceLineCorpus  # 한개로 통합된 문서 데이터 분리\n",
    "from soynlp.word import WordExtractor  # 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 문서 : 30091개\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터 문서 분리\n",
    "corpus = DoublespaceLineCorpus(file_name)\n",
    "print(f'훈련 데이터 문서 : {len(corpus)}개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 223348\n",
      "all branching entropies was computed # words = 361598\n",
      "all accessor variety was computed # words = 361598\n"
     ]
    }
   ],
   "source": [
    "# Sonlpy 학습 진행\n",
    "word_extractor = WordExtractor()\n",
    "# 학습 진행하며\n",
    "word_extractor.train(corpus)\n",
    "# 단어별 점수표 추출\n",
    "word_score_table = word_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] = 카\n",
      "[1] = 핏\n",
      "[2] = 뾰\n",
      "[3] = 뭡\n",
      "[4] = 캉\n",
      "[5] = 혼\n",
      "[6] = 삭\n",
      "[7] = 왓\n",
      "[8] = 펴\n",
      "[9] = 셋\n",
      "[10] = 뚜\n",
      "[11] = 박\n",
      "[12] = 2\n",
      "[13] = 쉴\n",
      "[14] = 움\n",
      "[15] = 뛴\n",
      "[16] = 우\n",
      "[17] = 얼\n",
      "[18] = 투\n",
      "[19] = 찜\n",
      "[20] = 막\n",
      "[21] = 뒤\n",
      "[22] = 윙\n",
      "[23] = 렛\n",
      "[24] = 지\n",
      "[25] = 넴\n",
      "[26] = 갉\n",
      "[27] = 함\n",
      "[28] = 김\n",
      "[29] = 휴\n",
      "[30] = 붙\n"
     ]
    }
   ],
   "source": [
    "# 단어별 점수표 확인\n",
    "for idx, key in enumerate(word_score_table.keys()):\n",
    "    print(f'[{idx}] = {key}')\n",
    "    if idx == 30: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 응집 확률(cohesion probability) : 내부 문자열(substring)이 얼마나 응집하여 자주 등장하는지를 판단하는 척도  \n",
    "==> 원리 : 문자열을 문자 단위로 분리, 왼쪽부터 순서대로 문자를 추가  \n",
    "==> 각 문자열이 주어졌을 때 그 다음 문자가 나올 확률을 계산/ 누적곱 한 값  \n",
    "==> 값이 높을수록 전체 코퍼스에서 이 문자열 시퀀스는 하나의 단어로 등장할 가능성 높음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06393648140409527"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table['바다'].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11518621707955429"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table['바다에'].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07716779358040307"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table['바다를'].cohesion_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SOYNLP의 L tokenzier  \n",
    "==> 띄어쓰기 단위로 나눈 어절 토큰 : L토큰 + R토큰  \n",
    "==> 예 : '공원에' => '공원' + '에', '공부하는' => '공부' + '하는'  \n",
    "==> 분리 기준 : 점수가 가장 높은 L토큰을 찾아내는 원리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('국제사회', '와'), ('우리', '의'), ('노력', '들로'), ('범죄', '를'), ('척결', '하자')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "# 토큰으로 쪼개기위한 L토큰 기준값\n",
    "scores = {word : score.cohesion_forward for word, score in word_score_table.items()}\n",
    "\n",
    "l_tokernizer = LTokenizer(scores = scores)\n",
    "l_tokernizer.tokenize('국제사회와 우리의 노력들로 범죄를 척결하자', flatten = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최대 점수 토크나이저  \n",
    "==> 띄어쓰기가 되어 있지 않은 문장에서 점수가 높은 글자 시퀀스를 순차적으로 찾아내는 토크나이저  \n",
    "==> 띄어쓰기가 되어 있지 않은 문장을 넣어서 점수를 통해 토큰화 된 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['국제사회', '와', '우리', '의', '노력', '들로', '범죄', '를', '척결', '하자']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "\n",
    "maxscore_tokenizer = MaxScoreTokenizer(scores = scores)\n",
    "maxscore_tokenizer.tokenize('국제사회와우리의노력들로범죄를척결하자')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['국제사회', '와', '우리', '의', '노력', '들로', '범죄', '를', '척결', '하자']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxscore_tokenizer('국제사회와 우리의 노력들로 범죄를 척결하자')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.07856876882976202)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table['국'].cohesion_forward, word_score_table['국제'].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09217735975351507, 0.20075093164820865)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table['국제사'].cohesion_forward, word_score_table['국제사회'].cohesion_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOYNLP를 이용한 반복되는 문자 정제  \n",
    "==> ㅋㅋ, ㅎㅎ 등의 이모티콘의 경우 불필요하게 연속되는 경우 많음  \n",
    "==> 반복되는 것은 하나로 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "앜^^^^^^^\n",
      "아ㅋ영화존잼쓰ㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n"
     ]
    }
   ],
   "source": [
    "from soynlp.normalizer import *\n",
    "import string\n",
    "print(string.punctuation)\n",
    "\n",
    "print(emoticon_normalize('앜^^^^^^^', num_repeats = 1))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ', num_repeats = 1))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠ', num_repeats = 2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠ', num_repeats = 2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠㅠㅠ', num_repeats = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting Customized_konlpy\n",
      "  Downloading customized_konlpy-0.0.64-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: Jpype1>=0.6.1 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from Customized_konlpy) (1.4.1)\n",
      "Requirement already satisfied: konlpy>=0.4.4 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from Customized_konlpy) (0.6.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from Jpype1>=0.6.1->Customized_konlpy) (23.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from konlpy>=0.4.4->Customized_konlpy) (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from konlpy>=0.4.4->Customized_konlpy) (1.24.3)\n",
      "Downloading customized_konlpy-0.0.64-py3-none-any.whl (881 kB)\n",
      "   ---------------------------------------- 0.0/881.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 10.2/881.5 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 112.6/881.5 kB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 573.4/881.5 kB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 881.5/881.5 kB 7.0 MB/s eta 0:00:00\n",
      "Installing collected packages: Customized_konlpy\n",
      "Successfully installed Customized_konlpy-0.0.64\n"
     ]
    }
   ],
   "source": [
    "# !pip install Customized_konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\Torch_NLP38\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['은', '경이', '는', '사무실', '로', '갔습니다', '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "\n",
    "twitter = Twitter()\n",
    "twitter.morphs('은경이는 사무실로 갔습니다.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기에 사전 추가\n",
    "twitter.add_dictionary('은경이', 'Noun')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
