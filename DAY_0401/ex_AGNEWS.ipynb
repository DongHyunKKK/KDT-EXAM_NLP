{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torchtext 라이브러리로 텍스트 분류 <hr>\n",
    "- 1단계 : 데이터 전처리 => 숫자형식으로 변환\n",
    "- 2단계 : 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1-1] 데이터 준비 => 내장 데이터셋 활용\n",
    "- AGNEWS 데이터셋 반복자 : 레이블(label) + 문장의 튜풀(tuple) 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchdata\n",
      "  Downloading torchdata-0.7.1-cp38-cp38-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from torchdata) (2.1.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from torchdata) (2.31.0)\n",
      "Requirement already satisfied: torch>=2 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from torchdata) (2.2.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from torch>=2->torchdata) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from torch>=2->torchdata) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from torch>=2->torchdata) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from torch>=2->torchdata) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from torch>=2->torchdata) (3.1.3)\n",
      "Collecting fsspec (from torch>=2->torchdata)\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from requests->torchdata) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from requests->torchdata) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from requests->torchdata) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from jinja2->torch>=2->torchdata) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from sympy->torch>=2->torchdata) (1.3.0)\n",
      "Downloading torchdata-0.7.1-cp38-cp38-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/1.3 MB 656.4 kB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.1/1.3 MB 1.0 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.3/1.3 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.5/1.3 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.8/1.3 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.3/1.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "   ---------------------------------------- 0.0/172.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 172.0/172.0 kB 10.1 MB/s eta 0:00:00\n",
      "Installing collected packages: fsspec, torchdata\n",
      "Successfully installed fsspec-2024.3.1 torchdata-0.7.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.17.2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchtext\n",
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "# DataPipe 타입 >>> iterator 타입 형변환\n",
    "train_iter = iter(AG_NEWS(split = 'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 확인 => (label, text), label : 1 ~ 4\n",
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 데이터 전처리 파이프라인 준비\n",
    "- 어휘집(vocab), 단어 벡터(word vector), 토크나이저(tokenizer)\n",
    "- 가공되지 않은 텍스트 문자열에 대한 데이터 처리 빌딩 블록\n",
    "- 일반적인 NLP 데이터 처리\n",
    "    - 첫번째 단계 : 가공되지 않은 학습 데이터셋으로 어휘집 생성  \n",
    "    ==> 토큰 목록 또는 반복자 받는 내장 팩토리 함수 (factory function) : ``bulid_vocab_from_iterator``\n",
    "    - 사용자는 어휘집에 추가할 특수 기호(special symbol) 전달 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토커나이즈 생성\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# 뉴스 학습 데이터 추출\n",
    "train_iter = AG_NEWS(split = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 제너레이터 함수 : 데이터 추출하여 토큰화\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        # 라벨, 텍스트 => 텍스트 토큰화\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어사전 생성\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials = ['<unk>'])\n",
    "\n",
    "# <UNK> 인덱스 0으로 설정\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 475, 21, 30, 5297]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['<unk>', 'here', 'is', 'an', 'example'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 => 정수 인코딩\n",
    "text_pipeline = lambda x : vocab(tokenizer(x))\n",
    "\n",
    "# 레이블 => 정수 인코딩\n",
    "label_pipeline = lambda x : int(x) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] 데이터 배치(batch)와 반복자 생성\n",
    "- torch.utils.data.DataLoader : getitem(), len() 구현한 맵 형태(map-style)\n",
    "- collate_fn() : DataLoader로부터 생성된 샘플 배치 함수  \n",
    "==> 입력 : DataLoader에 배치 크기(batch size)가 있는 배치(batch) 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 배치 크기만큼 데이터셋 반환 함수\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "\n",
    "    # 1개씩 뉴스기사, 라벨 추출해서 저장\n",
    "    for (_label, _text) in batch:\n",
    "        # 라벨 인코딩 후 저장\n",
    "        label_list.append(label_pipeline(_label))\n",
    "\n",
    "        # 텍스트 인코딩 후 저장\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype = torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "\n",
    "        # 텍스트 offset 즉, 텍스트 크기/길이 저장\n",
    "        offsets.append(processed_text.size(0))\n",
    "    \n",
    "    label_list = torch.tensor(label_list, dtype = torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim = 0)\n",
    "    text_list = torch.cat(text_list)\n",
    "\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split = 'train')\n",
    "dataloader = DataLoader(train_iter,\n",
    "                        batch_size = 8,\n",
    "                        shuffle = True,\n",
    "                        collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num class : 4   vocab size : 95811\n"
     ]
    }
   ],
   "source": [
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f'num class : {num_class}   vocab size : {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 1, 2, 1, 0, 0, 3]) tensor([ 1756,  8246,  2932,  1173,     3,  2090,    66,   117,  4407,    13,\n",
      "           27,    14,    15,     5,   928,  1756,  4556,  5265,     8,  2287,\n",
      "         2779,     7,   529,  1173,    10,    56,   167, 49357,    66,   117,\n",
      "          390,   997,     6,   686,     6,   102,    39,    81,  7033,     1,\n",
      "         4387,  5702, 18568,   454,  3149,    10,  6869,    92,    13,  4895,\n",
      "           14,  1146,     1,   640,    13,  4685,    14,    53,  4387,  5702,\n",
      "            3,     2,    50,    12,     9,   356,  2887,    54,     3,    46,\n",
      "           72,  2636,  4853,   153,    19,   622,   486,  2178,  6869,    92,\n",
      "            8,    26,    25,   104,   465,    19,   389,    19,  1284,   139,\n",
      "            4,   360,     3,   334,   252,   411, 17149,     1,  1453,   444,\n",
      "         3246,  6353,     4,  1504,    13,    31,    14,    31,    15,   835,\n",
      "         1453,  2166,   717,   343,  2990,  6353,    57,     4,   478,     6,\n",
      "        13847,     5,  3750,  2077,    20,     2, 13104,   138,  1184,     7,\n",
      "          450,     1,   272,  9411,   345,   477,  7981,   406,    13,    27,\n",
      "           14,    15,     2,   272,  3470,  1143,    10,    57,     3,  1530,\n",
      "            4,  8991,     5,  3727,  4619,    29, 38347,  2280,    63,     2,\n",
      "          739,   212,    10, 16427,   477, 17338,     4,     2,    88,   159,\n",
      "            1,  1802,   402,   144,   149,   270,    12,     9,  3369,  1802,\n",
      "          908, 25955,    32,    89,   350,     6,     2,    36,   149,    18,\n",
      "            5,  6395,  1859,  1435,   109,    38, 13564, 13640,     4,  3033,\n",
      "            2, 35068,     3,   162,    13, 57403,     3, 30528,    14,   144,\n",
      "          149,     1,   542,   653,  1426,    38,   464,  4101,     2,  1843,\n",
      "          100,    28,   212,  1426,     7,   530,   286,  1670,     4,   152,\n",
      "            2,   464,   870,     3,   510,    12,     9,   396,   749,  2987,\n",
      "         4101,    26,    57,   220,     5,  1569,     4,     2,  1443, 39589,\n",
      "         2427,  1169,     7,   529,   464,     1,   631,  2210,  6972, 29437,\n",
      "          631,  2210,  5773,    63, 68597,  6972,   147,  1387,     4,    37,\n",
      "         2204,     7,     2,   460,     6,  8620,     1,   952,  9014,   764,\n",
      "        57476,     3,    70,    13,  3852,    14,    53,   786,     6,    70,\n",
      "           16,     9,  1277,   100,   443,    30, 14078, 80763,    55,   177,\n",
      "          825,    34,     2,    88,   159,  1348,     2,   407,    16, 86038,\n",
      "            3,    48,   301,   350,     6,  1969,     1, 57530,    24,    70,\n",
      "           16,     9,  1277,   199,   125,  5493,  1986,     3,   871,  1592,\n",
      "            6,     2, 48534,  2383,     5,  1186,    10,     2, 47271,     8,\n",
      "         2363,     4,  2774,    18, 39623, 59782,     1,   208,  7870,  7790,\n",
      "            2,   431,   646,   245,     1, 57519,   764,   185,    18,   750,\n",
      "          711,  1878,  6835,  2396,   764,  1507,   185,     8,   222, 58182,\n",
      "         5512,     6,  1434,  3855,    13,   373,   287,  1002,     7,  1984,\n",
      "            3,  2619,     3, 50397,     3,   379,     3,  1984,     8,   339,\n",
      "           14,     1, 41983,  1651,    59,  1129,    23, 24599,   100,   764,\n",
      "         1922,     2,  3702,  4668,    11,   825,   764,  4531]) tensor([  0,  40,  98, 132, 171, 212, 256, 277])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\Torch_NLP38\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "for labels, texts, offsets in dataloader:\n",
    "    print(labels, texts, offsets)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
